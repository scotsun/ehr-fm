{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a59ef5-8613-4d8b-88ad-263c9b532045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f14a055-0695-4a31-8f6e-57bf3b333b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from tokenizers import Tokenizer\n",
    "from src.utils.data_utils import SeqSet, Seq, random_masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a93ba4-8981-4f15-81ab-05520245a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer.from_file(\n",
    "    \"./dataset/instacart/data/tk.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb97a05-a068-48ad-b43e-7fc8b728fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import FMConfig\n",
    "from src.utils.model_utils import build_model\n",
    "from src.utils.train_utils import (\n",
    "    load_cfg,\n",
    "    build_trainer,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553da957-0173-402a-ac07-d6afe6ab8e69",
   "metadata": {},
   "source": [
    "### bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e05a202-8ace-4df9-85dd-cf77f4d7bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instacart = Seq(\n",
    "    tokenizer=tk,\n",
    "    data_root=\"./dataset/instacart/data\",\n",
    "    data_folder=\"./dataset/instacart/data/instacart.parquet\",\n",
    "    max_seq=2048,\n",
    "    downstream_task_cohort=None,\n",
    "    outcome_vars=None,\n",
    "    time_operation=lambda x: x[\"t\"],\n",
    "    seq_id_col=\"user_id\",\n",
    "    set_id_col=\"order_number\",\n",
    "    token_col=\"product_id\",\n",
    "    additional_cols=[\"t\"]\n",
    ")\n",
    "train, valid = random_split(\n",
    "    dataset=instacart,\n",
    "    lengths=[0.9, 0.1],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9e8627b-9871-45fb-a656-290d2e665057",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73e1bfb-2f7e-4e19-9cb6-52b7f98fc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = load_cfg(\"./config/instacart_bert.yaml\")\n",
    "tk = Tokenizer.from_file(f\"./{cfg_dict['model']['tokenizer']}\")\n",
    "\n",
    "cfg = FMConfig(\n",
    "    vocab_size=tk.get_vocab_size(),\n",
    "    dataset=cfg_dict[\"dataset\"],\n",
    "    trainer=cfg_dict[\"trainer\"],\n",
    "    **cfg_dict[\"model\"],\n",
    ")\n",
    "model = build_model(cfg, \"FMBert\", device)\n",
    "trainer = build_trainer(cfg, model, tk, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4f11ed-7965-4b64-826c-0409c88cf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 12, 2048, 64])\n",
      "torch.Size([16, 12, 2048, 64])\n",
      "torch.Size([16, 12, 2048, 64])\n",
      "torch.Size([16, 12, 2048, 64])\n",
      "torch.Size([16, 12, 2048, 64])\n",
      "torch.Size([16, 12, 2048, 64])\n",
      "Total FLOPs: 6285.06 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "with FlopCounterMode(display=False) as flop_counter:\n",
    "    logits, h = model(\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        batch[\"attention_mask\"].to(device),\n",
    "        batch[\"t\"].to(device),\n",
    "    )\n",
    "    \n",
    "total_flops = flop_counter.get_total_flops()\n",
    "print(f\"Total FLOPs: {total_flops / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total FLOPs: {total_flops / (2048 * 16) / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5680b-8b32-43ac-abc8-9ad80cc3bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "for param in model.parameters():\n",
    "    num_params += param.numel()\n",
    "print(f\"Total number of parameters: {num_params / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac2392-5909-495a-a1c7-7ec6ac206a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_time_list = []\n",
    "for _ in tqdm(range(30)):\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    t = batch[\"t\"].to(device)\n",
    "    \n",
    "    masked_input_ids, labels = random_masking(\n",
    "        input_ids.clone(), tk, 0.2\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    logits, _ = model(\n",
    "        input_ids=masked_input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        t=t,\n",
    "    )\n",
    "    loss = CrossEntropyLoss(ignore_index=-100)(\n",
    "        logits.view(-1, logits.size(-1)), labels.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    step_time = t1 - t0\n",
    "\n",
    "    step_time_list.append(step_time)\n",
    "    \n",
    "    del logits, _, batch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa619e7c-3b92-4f86-a474-63344387bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean step time: {np.mean(step_time_list):.2f} sec\")\n",
    "print(f\"Std  step time: {np.std(step_time_list):.2f} sec\")\n",
    "print(f\"Mean throughput: {(2048 / np.array(step_time_list)).mean():.2f} #tokens/sec\")\n",
    "print(f\"Std  throughput: {(2048 / np.array(step_time_list)).std():.2f} #tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a6a49",
   "metadata": {},
   "source": [
    "#### Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83d042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = load_cfg(\"./config/instacart_longformer.yaml\")\n",
    "tk = Tokenizer.from_file(f\"./{cfg_dict['model']['tokenizer']}\")\n",
    "\n",
    "cfg = FMConfig(\n",
    "    vocab_size=tk.get_vocab_size(),\n",
    "    dataset=cfg_dict[\"dataset\"],\n",
    "    trainer=cfg_dict[\"trainer\"],\n",
    "    **cfg_dict[\"model\"],\n",
    ")\n",
    "model = build_model(cfg, \"FMLongformer\", device)\n",
    "trainer = build_trainer(cfg, model, tk, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3611c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e1d379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2048, 12, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (2048) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflop_counter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlopCounterMode\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m FlopCounterMode(display=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m flop_counter:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     logits, h = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m total_flops = flop_counter.get_total_flops()\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal FLOPs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_flops\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GFLOPs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\ehr-fm\\src\\models\\longformer.py:313\u001b[39m, in \u001b[36mFMLongformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, t)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    311\u001b[39m     \u001b[38;5;28mself\u001b[39m, input_ids: torch.Tensor, attention_mask: torch.Tensor, t: torch.Tensor\n\u001b[32m    312\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.lm_head(h)\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# (batch, max_seq, max_set_size, d_model)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\ehr-fm\\src\\models\\longformer.py:325\u001b[39m, in \u001b[36mFMLongformer.encode\u001b[39m\u001b[34m(self, input_ids, attention_mask, t)\u001b[39m\n\u001b[32m    323\u001b[39m h = \u001b[38;5;28mself\u001b[39m.t2v(t)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     h = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m h = \u001b[38;5;28mself\u001b[39m.last_norm(h)\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\ehr-fm\\src\\models\\longformer.py:261\u001b[39m, in \u001b[36mLongformerBlock.forward\u001b[39m\u001b[34m(self, x, attention_mask, global_attention_mask)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask, global_attention_mask):\n\u001b[32m    256\u001b[39m     attention_mask = \u001b[38;5;28mself\u001b[39m._merge_to_attention_mask(\n\u001b[32m    257\u001b[39m         attention_mask, global_attention_mask\n\u001b[32m    258\u001b[39m     )\n\u001b[32m    259\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connection0(\n\u001b[32m    260\u001b[39m         x,\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlongformer_self_attn_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_index_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_index_masked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_global_attn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.residual_connection1(x, \u001b[38;5;28mself\u001b[39m.ffn_block(x))\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:16\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions, head_mask)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1857\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1856\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1859\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1860\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1861\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1805\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1802\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1803\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1807\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1808\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1809\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1810\u001b[39m     ):\n\u001b[32m   1811\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\ehr-fm\\src\\layers\\pe.py:46\u001b[39m, in \u001b[36mRoPE.forward\u001b[39m\u001b[34m(self, x, time)\u001b[39m\n\u001b[32m     42\u001b[39m x_rope, x_pass = x[..., : \u001b[38;5;28mself\u001b[39m.d], x[..., \u001b[38;5;28mself\u001b[39m.d :]\n\u001b[32m     43\u001b[39m neg_half = torch.cat(\n\u001b[32m     44\u001b[39m     [-x_rope[..., \u001b[38;5;28mself\u001b[39m.d // \u001b[32m2\u001b[39m :], x_rope[..., : \u001b[38;5;28mself\u001b[39m.d // \u001b[32m2\u001b[39m]], dim=-\u001b[32m1\u001b[39m\n\u001b[32m     45\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m x_rope = \u001b[43mx_rope\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos_rotery\u001b[49m + neg_half * sin_rotery\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([x_rope, x_pass], dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\utils\\flop_counter.py:791\u001b[39m, in \u001b[36m_FlopCounterMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# no further decomposition; execute & count flops\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.counter._count_flops(func._overloadpacket, out, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\16677\\anaconda3\\envs\\research-new\\Lib\\site-packages\\torch\\_ops.py:756\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (12) must match the size of tensor b (2048) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "with FlopCounterMode(display=False) as flop_counter:\n",
    "    logits, h = model(\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        batch[\"attention_mask\"].to(device),\n",
    "        batch[\"t\"].to(device),\n",
    "    )\n",
    "    \n",
    "total_flops = flop_counter.get_total_flops()\n",
    "print(f\"Total FLOPs: {total_flops / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1cc0f-046d-4aff-9441-b88b7d47d71c",
   "metadata": {},
   "source": [
    "### base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2c059-05a3-42bd-899d-b27855e4cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "instacart = SeqSet(\n",
    "    tokenizer=tk,\n",
    "    data_root=\"./dataset/instacart/data\",\n",
    "    data_folder=\"./dataset/instacart/data/instacart.parquet\",\n",
    "    max_seq=64,\n",
    "    max_set_size=32,\n",
    "    downstream_task_cohort=None,\n",
    "    outcome_vars=None,\n",
    "    time_operation=lambda x: x[\"t\"],\n",
    "    seq_id_col=\"user_id\",\n",
    "    set_id_col=\"order_number\",\n",
    "    token_col=\"product_id\",\n",
    "    additional_cols=[\"t\"]\n",
    ")\n",
    "train, valid = random_split(\n",
    "    dataset=instacart,\n",
    "    lengths=[0.9, 0.1],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea4365-d703-47fb-97d0-774877d4d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_dict = load_cfg(\"./config/instacart_base.yaml\")\n",
    "tk = Tokenizer.from_file(f\"./{cfg_dict['model']['tokenizer']}\")\n",
    "\n",
    "cfg = FMConfig(\n",
    "    vocab_size=tk.get_vocab_size(),\n",
    "    dataset=cfg_dict[\"dataset\"],\n",
    "    trainer=cfg_dict[\"trainer\"],\n",
    "    **cfg_dict[\"model\"],\n",
    ")\n",
    "model = build_model(cfg, \"FMBase\", device)\n",
    "trainer = build_trainer(cfg, model, tk, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb526d54-7903-4b23-8fbb-7f80906263d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb252a6e-c8f5-4c75-a155-e94165e53cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "with FlopCounterMode(display=False) as flop_counter:\n",
    "    logits, h = model(\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        batch[\"attention_mask\"].to(device),\n",
    "        batch[\"set_attention_mask\"].to(model.device),\n",
    "        batch[\"t\"].to(device),\n",
    "    )\n",
    "    \n",
    "total_flops = flop_counter.get_total_flops()\n",
    "print(f\"Total FLOPs: {total_flops / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total FLOPs: {total_flops / (2048 * 16) / 1e9:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99701482-228f-415b-a107-71587a20899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "for param in model.parameters():\n",
    "    num_params += param.numel()\n",
    "print(f\"Total number of parameters: {num_params / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9b98d-cbac-4e44-971c-d685373683dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_time_list = []\n",
    "for _ in tqdm(range(30)):\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    set_attention_mask = batch[\"set_attention_mask\"].to(device)\n",
    "    t = batch[\"t\"].to(device)\n",
    "    \n",
    "    masked_input_ids, labels = random_masking(\n",
    "        input_ids.clone(), tk, 0.2\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    logits, _ = model(\n",
    "        input_ids=masked_input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        set_attention_mask=set_attention_mask,\n",
    "        t=t,\n",
    "    )\n",
    "    loss = CrossEntropyLoss(ignore_index=-100)(\n",
    "        logits.view(-1, logits.size(-1)), labels.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    step_time = t1 - t0\n",
    "\n",
    "    step_time_list.append(step_time)\n",
    "    \n",
    "    del logits, _, batch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be7cae-c275-4a2b-9485-6ed0a253ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean step time: {np.mean(step_time_list):.2f} sec\")\n",
    "print(f\"Std  step time: {np.std(step_time_list):.2f} sec\")\n",
    "print(f\"Mean throughput: {(2048 / np.array(step_time_list)).mean():.2f} #tokens/sec\")\n",
    "print(f\"Std  throughput: {(2048 / np.array(step_time_list)).std():.2f} #tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c6f91-591b-4fef-a4b1-354449e36121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
